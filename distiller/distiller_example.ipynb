{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [1, 512], got [1, 9]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/john/Desktop/MSc AI/DL4NLP/Project/distiller/distiller_example.ipynb Cell 1\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mpad_token_id)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits, decoder_input_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(logits\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [1, 512], got [1, 9]"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import argparse\n",
    "import wandb\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.models.t5.modeling_t5 import T5Stack, T5ForConditionalGeneration, T5Config\n",
    "from transformers import T5Model\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from datamodules.mt_distillation_datamodule import MTDistillationDatamodule\n",
    "\n",
    "# model_checkpoint = \"t5-small\"\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = T5Model(config=T5Config.from_pretrained(\"t5-small\"))\n",
    "sentence = \"translate English to Romanian: Hello, my dog is cute\"\n",
    "label = \"Salut, c√¢inele meu este frumos\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "inputs = inputs[\"input_ids\"]\n",
    "decoder_input_ids = tokenizer(label, return_tensors=\"pt\")\n",
    "decoder_input_ids = decoder_input_ids[\"input_ids\"]\n",
    "decoder_attention_mask = decoder_input_ids.ne(tokenizer.pad_token_id).long()\n",
    "outputs = model(input_ids=inputs, decoder_input_ids=decoder_input_ids, attention_mask=attention_mask, decoder_attention_mask=decoder_attention_mask)\n",
    "\n",
    "logits = outputs[0]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "loss = criterion(logits.permute(0, 2, 1), decoder_input_ids)\n",
    "print(loss)\n",
    "print(logits.shape)\n",
    "print(logits)\n",
    "print(tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 25801 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/john/Desktop/MSc AI/DL4NLP/Project/distiller/distiller_example.ipynb Cell 2\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m), decoder_input_ids)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m logits\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 25801 is out of bounds."
     ]
    }
   ],
   "source": [
    "loss = criterion(logits.permute(0, 2, 1), decoder_input_ids)\n",
    "logits.permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration din0s--ccmatrix_en-ro-b5eb1c3b243f258f\n",
      "Reusing dataset parquet (/home/john/.cache/huggingface/datasets/din0s___parquet/din0s--ccmatrix_en-ro-b5eb1c3b243f258f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f164891b957c49f18808dd319e698f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/john/.cache/huggingface/datasets/din0s___parquet/din0s--ccmatrix_en-ro-b5eb1c3b243f258f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-246f20ddcdebc8fd.arrow\n",
      "Using custom data configuration j0hngou--ccmatrix_en-fr-c75b778ef0fed3ea\n",
      "Reusing dataset parquet (/home/john/.cache/huggingface/datasets/j0hngou___parquet/j0hngou--ccmatrix_en-fr-c75b778ef0fed3ea/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168e76c3fb634de5966189970e266b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/john/.cache/huggingface/datasets/j0hngou___parquet/j0hngou--ccmatrix_en-fr-c75b778ef0fed3ea/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-7b6fd4d61823253e.arrow\n",
      "Using custom data configuration j0hngou--ccmatrix_de-en-2acfbf58505ec757\n",
      "Reusing dataset parquet (/home/john/.cache/huggingface/datasets/j0hngou___parquet/j0hngou--ccmatrix_de-en-2acfbf58505ec757/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0990e4d96b9d4736987df680dd1e54e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/john/.cache/huggingface/datasets/j0hngou___parquet/j0hngou--ccmatrix_de-en-2acfbf58505ec757/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-cd6cc0e29ebdabbb.arrow\n",
      "/home/john/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Module inputs don't match the expected format.\nExpected format: {'predictions': Value(dtype='string', id=None)},\nInput predictions: tensor([[    0,  5636,     3,     9,  6344,   111, 20398,  7529,     3,    32,\n           115, 11555,  3020,     6,     3,  7753,  1785, 15466,  1165,   142,\n             3,   122, 26784,   111,  1746,   111,  1314,   630,     5,     1,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [    0,  9605,  4650,    15,  1456,    15,   176,     3, 10365,  6694,\n           267,  5541,   185,   146,     7,    50,     3,    32, 21798,    98,\n         21473,   111,   124, 21285,    20,  3748,   185, 23286,   111, 28495,\n           400,   176,  1336,  3738, 17694,   176,  9975,     6,  3981, 23969,\n            20, 30562,    98,    20,  3089,   111,  9047,   267,    20, 31047,\n           554,  1314,    15,    26,  2429,  2878, 19783, 24547,     7,   276,\n           154,  2638,    41,    60, 21749,     3,     7,     9,   111, 10414,\n            61,   191,  4301,   413,  5904,   111,  8388,     5,     1],\n        [    0,  4908, 12411,  1109,   400,   815,   361,     3,    26,    98,\n           202,  9097,   111,  9482, 15063,     1,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [    0,  5895,  1778,   375,  2515,   440,  1074,  5232,     9,   626,\n            98,   249, 19468,    98,   191,  2631, 20101,   267,   375,     3,\n         20553, 16916,   206,  1557,   246,  1479,     3,    32, 25767, 13232,\n            98,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/john/Desktop/MSc AI/DL4NLP/Project/distiller/distiller_example.ipynb Cell 2\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m first_batch \u001b[39m=\u001b[39m first_batch[\u001b[39m'\u001b[39m\u001b[39men-ro\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39mfirst_batch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m], attention_mask\u001b[39m=\u001b[39mfirst_batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m], max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/distiller/distiller_example.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(perplexity\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49moutputs, references\u001b[39m=\u001b[39;49mfirst_batch[\u001b[39m'\u001b[39;49m\u001b[39mdecoder_input_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/evaluate/module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[1;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/evaluate/module.py:511\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    506\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions and/or references don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match the expected format.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    507\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected format: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_features \u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput predictions: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput references: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(references)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m     )\n\u001b[0;32m--> 511\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Module inputs don't match the expected format.\nExpected format: {'predictions': Value(dtype='string', id=None)},\nInput predictions: tensor([[    0,  5636,     3,     9,  6344,   111, 20398,  7529,     3,    32,\n           115, 11555,  3020,     6,     3,  7753,  1785, 15466,  1165,   142,\n             3,   122, 26784,   111,  1746,   111,  1314,   630,     5,     1,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [    0,  9605,  4650,    15,  1456,    15,   176,     3, 10365,  6694,\n           267,  5541,   185,   146,     7,    50,     3,    32, 21798,    98,\n         21473,   111,   124, 21285,    20,  3748,   185, 23286,   111, 28495,\n           400,   176,  1336,  3738, 17694,   176,  9975,     6,  3981, 23969,\n            20, 30562,    98,    20,  3089,   111,  9047,   267,    20, 31047,\n           554,  1314,    15,    26,  2429,  2878, 19783, 24547,     7,   276,\n           154,  2638,    41,    60, 21749,     3,     7,     9,   111, 10414,\n            61,   191,  4301,   413,  5904,   111,  8388,     5,     1],\n        [    0,  4908, 12411,  1109,   400,   815,   361,     3,    26,    98,\n           202,  9097,   111,  9482, 15063,     1,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [    0,  5895,  1778,   375,  2515,   440,  1074,  5232,     9,   626,\n            98,   249, 19468,    98,   191,  2631, 20101,   267,   375,     3,\n         20553, 16916,   206,  1557,   246,  1479,     3,    32, 25767, 13232,\n            98,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0]])"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type='metric')\n",
    "dm = MTDistillationDatamodule(batch_size=8)\n",
    "dm.setup()\n",
    "first_batch = next(iter(dm.train_dataloader()))\n",
    "# first_batch = {k: v.to(model.device) for k, v in first_batch.items()}\n",
    "first_batch = first_batch['en-ro']\n",
    "outputs = model.generate(input_ids=first_batch['input_ids'], attention_mask=first_batch['attention_mask'], max_length=128)\n",
    "# predictions = {'predictions': predict, id=None)}\n",
    "print(perplexity.compute(predictions=outputs, references=first_batch['decoder_input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[13959,  1566,    12,  3871,    29,    10,   938,   352,   441,    62,\n",
       "             54,  1984,   959,     6,    38,    66,    13,     8,  4269,    33,\n",
       "            435,   441,  3242,     5,     1,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [13959,  1566,    12,  3871,    29,    10,  9071, 15093,     7,    16,\n",
       "              8,  6694,     7,    11,  5541,     7,  2237,    12,     3,     9,\n",
       "           1827,  5362,    16,    84,  3986,  3977,    16,     8,  1336,  3738,\n",
       "          17694,     3, 12884,     7,    13,  9975,     6,   192, 13090,  4897,\n",
       "              7,     3,    26,    22,  7795,    16,  9047,     6,    11,     8,\n",
       "           4840,    15,  1836,   297,    13,  1661, 19783, 24547,     7,   276,\n",
       "            154,  2638,    41,    60,    18, 19971,    16, 10414,    61,    21,\n",
       "          13100,    16,  8388,     5,     1],\n",
       "         [13959,  1566,    12,  3871,    29,    10,  4908, 12411,  1109,     7,\n",
       "             54,    36, 10947,    16,   128,  1488,     1,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [13959,  1566,    12,  3871,    29,    10,   101,   857,    24,   626,\n",
       "           2790,    19,   315,    21,   334,  2335,    11,    24,     3,  9582,\n",
       "             34,   174,    59,    36,     3,     9,  1444,  4393,     5,     1,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0]]),\n",
       " 'decoder_input_ids': tensor([[  391,  2398,  3398,    15,   212,  2631,   246, 12500,     3,    32,\n",
       "          16728,  7576,  5904,     6,   191,   375, 15466,  1165,   142,     3,\n",
       "            122, 26784,   111, 21544,  5662,     5,     1,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [ 9605,  1629,  1456,    98,   176,     3, 10365,  6694,   198,  5541,\n",
       "              3,     9,   146,     7,    50,     3,    32, 21798,    98, 21473,\n",
       "              6,   124,     3,     9, 15411,    17, 17066,     9,     3,     9,\n",
       "          21285,    20,  5029,   111, 28495,   400,   176,  1336,  3738, 17694,\n",
       "            176,  9975,     6,  5748,  4572,     9,    20, 30562,    98,    20,\n",
       "           3089,   176,  9047,   784,  4906,   908,   198,    20, 31047,   111,\n",
       "           8388,     3,     9, 28076,  2878, 19783, 24547,     7,   276,   154,\n",
       "           2638,    41,    60,  4529,   111, 10414,    61,   191,  4301,   413,\n",
       "           4673,     5,     1],\n",
       "         [ 4908,  5566,  8721,   109,   815,   361,     3,    26,    98,   202,\n",
       "           9097,   111,  9482, 15063,     1,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [    3, 10288,    16,  1983, 19822, 21909,     9,     6, 28596,  6850,\n",
       "             23, 21660,   673, 18000,    20,  6959,   123,  9787,     9, 24352,\n",
       "             16, 13636,     3,   104, 14688,  2038,     1,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0]]),\n",
       " 'decoder_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0- T5ForConditionalGeneration\n",
      "   1- Embedding\n",
      "   1- T5Stack\n",
      "      2- Embedding\n",
      "      2- ModuleList\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Embedding\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "      2- T5LayerNorm\n",
      "      2- Dropout\n",
      "   1- T5Stack\n",
      "      2- Embedding\n",
      "      2- ModuleList\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Embedding\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "         3- T5Block\n",
      "            4- ModuleList\n",
      "               5- T5LayerSelfAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerCrossAttention\n",
      "                  6- T5Attention\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "               5- T5LayerFF\n",
      "                  6- T5DenseActDense\n",
      "                     7- Linear\n",
      "                     7- Linear\n",
      "                     7- Dropout\n",
      "                     7- ReLU\n",
      "                  6- T5LayerNorm\n",
      "                  6- Dropout\n",
      "      2- T5LayerNorm\n",
      "      2- Dropout\n",
      "   1- Linear\n"
     ]
    }
   ],
   "source": [
    "def visualize_children(\n",
    "    object,\n",
    "    level : int = 0,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints the children of (object) and their children too, if there are any.\n",
    "    Uses the current depth (level) to print things in a ordonnate manner.\n",
    "    \"\"\"\n",
    "    print(f\"{'   ' * level}{level}- {type(object).__name__}\")\n",
    "    try:\n",
    "        for child in object.children():\n",
    "            visualize_children(child, level + 1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "visualize_children(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config PretrainedConfig {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 2,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 2,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Distiller import Distiller\n",
    "\n",
    "distiller = Distiller(model, 6)\n",
    "student = distiller.get_student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57714432"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student.num_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl4nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58d13f2f10dccb1fc6980cbdb204b9a9907a6ae2fdecbf08799c872bf73330f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
