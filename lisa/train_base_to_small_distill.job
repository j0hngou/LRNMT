#!/bin/bash

#SBATCH --partition=gpu_titanrtx_shared_course
#SBATCH --gres=gpu:1
#SBATCH --job-name=T5BtoT5S
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=6
#SBATCH --time=48:00:00
#SBATCH --mem=100000M
#SBATCH --output=lisa/outputs/dl4nlp_train_%A.out

module purge
module load 2021
module load Anaconda3/2021.05

source activate dl4nlp

cd distiller

python train_distiller.py --batch_size 48 --weight_decay 0.00 --lr 2e-4 --disable_dropout --fp16