{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1c2305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration din0s--ccmatrix_en-ro-b5eb1c3b243f258f\n",
      "Reusing dataset parquet (/home/john/.cache/huggingface/datasets/din0s___parquet/din0s--ccmatrix_en-ro-b5eb1c3b243f258f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a10fed525474e558f1c726c67ed2165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/john/.cache/huggingface/datasets/din0s___parquet/din0s--ccmatrix_en-ro-b5eb1c3b243f258f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-246f20ddcdebc8fd.arrow\n",
      "Using custom data configuration j0hngou--ccmatrix_en-fr-c75b778ef0fed3ea\n",
      "Reusing dataset parquet (/home/john/.cache/huggingface/datasets/j0hngou___parquet/j0hngou--ccmatrix_en-fr-c75b778ef0fed3ea/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de15c906374b4d2690d9db450b41fe9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/john/.cache/huggingface/datasets/j0hngou___parquet/j0hngou--ccmatrix_en-fr-c75b778ef0fed3ea/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-7b6fd4d61823253e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64200eacdbbb4d6889c3babde2c84dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6958776843bd4098973cda436c4023d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/dl4nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en-ro': {'source': tensor([[13959,  1566,    12,  3871,    29,    10, 15476,    13, 21268,   159,\n",
       "              51,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "          [13959,  1566,    12,  3871,    29,    10,  2449,    33,     3, 22765,\n",
       "              12,   103,    96, 23180,  1686,    68,    59,   762,     5,     1,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "          [13959,  1566,    12,  3871,    29,    10,    94,    19,  1500,    12,\n",
       "            1576,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'target': tensor([[  374,  5582,  5904, 19896,  6835,     1,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0],\n",
       "          [    3, 28073,   488,  5748,  3649,   246, 12500,  6573,     3,     6,\n",
       "               6,    75,  4721,  1686,   649,   206,  9324,     5,     1,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0],\n",
       "          [    3,     2,    23,     3,     7,    18,     9,  1312,    98,    52,\n",
       "            1439,    17, 19554,   864,     5,     1,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0]]),\n",
       "  'decoder_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " 'en-fr': {'source': tensor([[13959,  1566,    12,  2379,    10,   621,   578, 21761,  6436,   901,\n",
       "              51,    32,  8399,    15,     6,   258,   112, 14330,    16,     8,\n",
       "            6611,   107,    60,   115,     6,     3,    88,     3,  9000,    44,\n",
       "             166,    12,  1205,    12,     8,  1322,    13,  3352,     5,     1],\n",
       "          [13959,  1566,    12,  2379,    10,    37,   549,  4176,  6008,   963,\n",
       "               3,  4894,  1277,    41,  9456, 28755,  1277,     6, 17927,     6,\n",
       "           16646,     6,  1156,   387, 21459,     7,     6,   672,     5,   137,\n",
       "               1,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'target': tensor([[ 7658,  3517,  7683,    23,     3,    40,    22, 27525,   901,    51,\n",
       "              32,  8399,    15,     6,  4851,   520, 14330,   185,  6611,   107,\n",
       "              60,   115,     6,     3,   173,     3, 16000,    15,   247,    73,\n",
       "            2761,  1993,    20,  7268,   687,     3,    35, 10225,     3,    26,\n",
       "              22, 30290,     2,    40,     5,     1],\n",
       "          [  312,  6008,   549,  4176,   259, 23433, 20797,    20, 13097,    41,\n",
       "            3357,   154, 14018,    20, 27661,     6,     3,    17,    76,    63,\n",
       "            1724,     6, 13092,    15,     7,     6,     3,  9631,    49,  8239,\n",
       "               7,     3,    26,    22,  1607,  1156,     7, 16463,     1,     0,\n",
       "               0,     0,     0,     0,     0,     0]]),\n",
       "  'decoder_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datamodules.mt_distillation_datamodule import MTDistillationDatamodule\n",
    "\n",
    "dm = MTDistillationDatamodule(batch_size=5)\n",
    "dm.setup()\n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deafd0ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MTDistillationDatamodule' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/john/Desktop/MSc AI/DL4NLP/Project/dm_example.ipynb Cell 2\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/dm_example.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         examples[lcode][\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m labels\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/dm_example.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m examples\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/john/Desktop/MSc%20AI/DL4NLP/Project/dm_example.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m dm\u001b[39m.\u001b[39;49mmap(preprocess_function, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MTDistillationDatamodule' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "prefix_dict = {}\n",
    "prefix_dict['ro'] = \"translate English to Romanian: \"\n",
    "prefix_dict['it'] = \"translate English to Italian: \"\n",
    "prefix_dict['de'] = \"translate English to German: \"\n",
    "prefix_dict['fr'] = \"translate English to French: \"\n",
    "\n",
    "language_codes = ['en-ro', 'en-fr']\n",
    "\n",
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    for lcode in language_codes:\n",
    "        inputs = [prefix_dict[lcode] + ex[source_lang] for ex in examples[lcode][\"translation\"]]\n",
    "        targets = [ex[lcode] for ex in examples[lcode][\"translation\"]]\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "        # Setup the tokenizer for targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, truncation=True)[\"input_ids\"]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        examples[lcode][\"input_ids\"] = model_inputs[\"input_ids\"]\n",
    "        examples[lcode][\"attention_mask\"] = model_inputs[\"attention_mask\"]\n",
    "        examples[lcode][\"labels\"] = labels\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f81e115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 43])\n",
      "torch.Size([1, 43])\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"en-fr\"][\"source\"].shape)\n",
    "print(batch[\"en-ro\"][\"source\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5ad7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39d526b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-21.9249,  -8.9075,  -9.3580,  ..., -46.9724, -47.1455, -47.0554],\n",
       "         [-21.4823, -10.0551,  -4.1233,  ..., -42.9394, -43.0076, -42.9269],\n",
       "         [-23.4109,  -9.6241, -13.6325,  ..., -47.0220, -47.0796, -47.0454],\n",
       "         ...,\n",
       "         [-10.6927,  -4.4714,  -9.4419,  ..., -39.3290, -39.4041, -39.4781],\n",
       "         [-10.8941,  -4.4819,  -9.5339,  ..., -39.4958, -39.5699, -39.6404],\n",
       "         [-10.9117,  -4.4439,  -9.5682,  ..., -39.4095, -39.4821, -39.5508]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=batch[\"en-ro\"][\"source\"], \n",
    "      attention_mask=batch[\"en-ro\"][\"attention_mask\"], \n",
    "      decoder_input_ids=batch[\"en-ro\"][\"target\"],     \n",
    "      decoder_attention_mask=batch[\"en-ro\"][\"decoder_attention_mask\"]\n",
    "     ).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e65907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl4nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "58d13f2f10dccb1fc6980cbdb204b9a9907a6ae2fdecbf08799c872bf73330f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
